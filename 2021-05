2021-05-10

[ğŸŒŸ]Are Pre-trained Convolutions Better than Pre-trained Transformers?
æ ‡é¢˜ï¼šé¢„å…ˆè®­ç»ƒçš„å·ç§¯æ¯”é¢„å…ˆè®­ç»ƒçš„Transformeræ›´å¥½å—ï¼Ÿ
ä½œè€…ï¼šYi Tay,Mostafa Dehghani,Jai Gupta,Dara Bahri,Vamsi Aribandi,Zhen Qin,Donald Metzler
æœºæ„ï¼šGoogle Research, Brain Team, Amsterdam, Netherlands, Mountain View, California Mountain View, California Mountain View, California, Google research, Mountain view, California
å¤‡æ³¨ï¼šAccepted to ACL 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.03322
åŸºäºå·ç§¯çš„seq2seqèŒƒå¼çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ•ˆæœä¸è¾“tansformerï¼Œé€Ÿåº¦æ›´ä¼˜ã€‚

[ğŸŒŸğŸŒŸ]A Survey of Data Augmentation Approaches for NLP
æ ‡é¢˜ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ•°æ®å¢å¼ºæ–¹æ³•ç»¼è¿°
ä½œè€…ï¼šSteven Y. Feng,Varun Gangal,Jason Wei,Sarath Chandar,Soroush Vosoughi,Teruko Mitamura,Eduard Hovy
æœºæ„ï¼šCarnegie Mellon University,Google Research, Mila-Quebec AI Institute,Dartmouth College, soroushadartmouth. edu
å¤‡æ³¨ï¼šAccepted to ACL 2021 Findings
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.03075
https://blog.csdn.net/choose_c/article/details/116769154

2021-05-11

[ğŸŒŸğŸŒŸğŸŒŸ]ReadTwice: Reading Very Large Documents with Memories
æ ‡é¢˜ï¼šReadTwiceï¼šç”¨è®°å¿†é˜…è¯»éå¸¸å¤§çš„æ–‡æ¡£
ä½œè€…ï¼šYury Zemlyanskiy,Joshua Ainslie,Michiel de Jong,Philip Pham,Ilya Eckstein,Fei Sha
æœºæ„ï¼šUniversity of Southern California, Google Research
å¤‡æ³¨ï¼šTo appear in the proceedings of NAACL 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.04241
ä½¿ç”¨ä¸¤æ­¥èµ°çš„æ–¹æ¡ˆè¿›è¡Œé˜…è¯»ç†è§£æˆ–è€…æ˜¯QAï¼Œç¬¬ä¸€æ­¥ä½¿ç”¨è®°å¿†æ¨¡å—è®°å½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¬¬äºŒæ­¥åœ¨å…¶ä»–æ®µè½çš„æ—¶å€™ä½¿ç”¨ã€‚

[ğŸŒŸğŸŒŸ]Poolingformer: Long Document Modeling with Pooling Attention
æ ‡é¢˜ï¼šPoolingFormï¼šå…·æœ‰é›†ä¸­æ³¨æ„åŠ›çš„é•¿æ–‡æ¡£å»ºæ¨¡
ä½œè€…ï¼šHang Zhang,Yeyun Gong,Yelong Shen,Weisheng Li,Jiancheng Lv,Nan Duan,Weizhu Chen
å¤‡æ³¨ï¼šAccepted by ICML 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.04371
å¸¦æœ‰æ³¨æ„åŠ›poolingçš„é•¿æ–‡æ¡£å»ºæ¨¡é—®ç­”æ¨¡å‹ã€‚

2021-05-17

[ğŸŒŸ]Locate and Label: A Two-stage Identifier for Nested Named Entity  Recognition
æ ‡é¢˜ï¼šå®šä½å’Œæ ‡æ³¨ï¼šåµŒå¥—å‘½åå®ä½“è¯†åˆ«çš„ä¸¤é˜¶æ®µæ ‡è¯†ç¬¦
ä½œè€…ï¼šYongliang Shen,Xinyin Ma,Zeqi Tan,Shuai Zhang,Wen Wang,Weiming Lu
æœºæ„ï¼šCollege of Computer Science and Technology, Zhejiang University, University of Science and Technology of China
å¤‡æ³¨ï¼šAccepted to ACL 2021, submission version
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.06804
æ‘˜è¦ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNamed entity recognitionï¼ŒNERï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ã€‚ä¼ ç»Ÿçš„NERç ”ç©¶åªæ¶‰åŠå¹³é¢å®ä½“ï¼Œå¿½ç•¥äº†åµŒå¥—å®ä½“ã€‚
åŸºäºå¹¿åŸŸçš„æ–¹æ³•å°†å®ä½“è¯†åˆ«è§†ä¸ºå¹¿åŸŸåˆ†ç±»ä»»åŠ¡ã€‚è¿™äº›æ–¹æ³•è™½ç„¶å…·æœ‰å¤„ç†åµŒå¥—NERçš„èƒ½åŠ›ï¼Œä½†è®¡ç®—é‡å¤§ï¼Œå¯¹è¾¹ç•Œä¿¡æ¯çš„å¿½ç•¥ï¼Œå¯¹éƒ¨åˆ†åŒ¹é…å®ä½“çš„è·¨åº¦åˆ©ç”¨ä¸è¶³ï¼Œé•¿å®ä½“è¯†åˆ«å›°éš¾ã€‚
ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå®ä½“æ ‡è¯†ç¬¦ã€‚é¦–å…ˆé€šè¿‡å¯¹ç§å­è·¨åº¦è¿›è¡Œè¿‡æ»¤å’Œè¾¹ç•Œå›å½’æ¥ç”Ÿæˆè·¨åº¦å»ºè®®ä»¥å®šä½å®ä½“ï¼Œç„¶åç”¨ç›¸åº”çš„ç±»åˆ«æ ‡è®°è¾¹ç•Œè°ƒæ•´åçš„è·¨åº¦å»ºè®®ã€‚
è¯¥æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­å®ä½“å’Œéƒ¨åˆ†åŒ¹é…è·¨åº¦çš„è¾¹ç•Œä¿¡æ¯ã€‚é€šè¿‡è¾¹ç•Œå›å½’ï¼Œç†è®ºä¸Šå¯ä»¥è¦†ç›–ä»»æ„é•¿åº¦çš„å®ä½“ï¼Œæé«˜äº†å¯¹é•¿å®ä½“çš„è¯†åˆ«èƒ½åŠ›ã€‚
æ­¤å¤–ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µä¸­è¿‡æ»¤æ‰è®¸å¤šä½è´¨é‡çš„ç§å­è·¨åº¦ï¼Œé™ä½äº†æ¨ç†çš„æ—¶é—´å¤æ‚åº¦ã€‚åœ¨åµŒå¥—çš„NERæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æ¨¡å‹ã€‚

2021-05-18

[ğŸŒŸ]Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter
æ ‡é¢˜ï¼šåŸºäºBERTé€‚é…å™¨çš„è¯å…¸å¢å¼ºå‹ä¸­æ–‡åºåˆ—æ ‡æ³¨
ä½œè€…ï¼šWei Liu,Xiyan Fu,Yue Zhang,Wenming Xiao
æœºæ„ï¼šDAMO Academy, Alibaba Group, China, College of Computer Science, Nankai University, China, School of Engineering, Westlake University, China, Institute of Advanced Technology, Westlake Institute for Advanced Study
å¤‡æ³¨ï¼šaccepted by ACL2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.07148
æ‘˜è¦ï¼šç”±äºè¯åº“ä¿¡æ¯å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚BERTï¼‰å„è‡ªçš„ä¼˜åŠ¿ï¼Œå®ƒä»¬è¢«ç”¨æ¥æ¢ç´¢æ±‰è¯­åºåˆ—æ ‡æ³¨ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»…ä»…é€šè¿‡ä¸€ä¸ªæµ…å±‚éšæœºåˆå§‹åŒ–çš„åºåˆ—å±‚æ¥èåˆè¯æ±‡ç‰¹å¾ï¼Œè€Œæ²¡æœ‰å°†å®ƒä»¬é›†æˆåˆ°BERTçš„åº•å±‚ã€‚
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºä¸­æ–‡åºåˆ—æ ‡æ³¨çš„è¯åº“å¢å¼ºBERTï¼ˆLEBERTï¼‰ï¼Œå®ƒé€šè¿‡è¯åº“é€‚é…å™¨å±‚å°†å¤–éƒ¨è¯åº“çŸ¥è¯†ç›´æ¥é›†æˆåˆ°BERTå±‚ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹æœ‰åˆ©äºåœ¨BERTçš„åº•å±‚è¿›è¡Œæ·±å±‚è¯æ±‡çŸ¥è¯†èåˆã€‚
åœ¨åä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å‘½åå®ä½“è¯†åˆ«ã€åˆ†è¯å’Œè¯æ€§æ ‡æ³¨ä¸‰ä¸ªä»»åŠ¡çš„å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜LEBERTå–å¾—äº†æœ€æ–°çš„ç»“æœã€‚

2021-05-20

[ğŸŒŸ]Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation
æ ‡é¢˜ï¼šç”¨äºé—­åŸŸå¯¹è¯ç”Ÿæˆçš„æ£€ç´¢-å¢å¼ºå‹Transformer-XL
ä½œè€…ï¼šGiovanni Bonetta,Rossella Cancelliere,Ding Liu,Paul Vozila
æœºæ„ï¼šDepartment of Computer Science, University of Turin, Torino, Italy, Nuance Communications Inc., Burlington, MA, USA
å¤‡æ³¨ï¼šThe International FLAIRS Conference Proceedings volume 34 issue 1
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.09235
æ‘˜è¦ï¼šåŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­è¡¨ç°å‡ºå¾ˆå¥½çš„æ¨¡å¼å’Œç»“æ„æ•è·èƒ½åŠ›ï¼Œå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°çš„æˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„å¤šåŒå¯¹è¯å“åº”ç”Ÿæˆæ¨¡å‹ã€‚
æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŸºäºä¸€ç§æ··åˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€ç§æ–°çš„æ£€ç´¢æœºåˆ¶æ¥æ‰©å……åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æœºåˆ¶é€šè¿‡kè¿‘é‚»æœç´¢æ¥åˆ©ç”¨è®­ç»ƒæ•°æ®ä¸­çš„è®°å¿†ä¿¡æ¯ã€‚
æˆ‘ä»¬çš„ç³»ç»Ÿæ˜¯åœ¨ä¸¤ä¸ªç”±å®¢æˆ·/åŠ©ç†å¯¹è¯æ¡†åˆ¶ä½œçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°çš„ï¼šTaskmaster-1ï¼Œç”±Googleå‘å¸ƒï¼Œæ‹¥æœ‰é«˜è´¨é‡ã€é¢å‘ç›®æ ‡çš„ä¼šè¯æ•°æ®å’Œä»çœŸå®çš„å®¢æˆ·æœåŠ¡å‘¼å«ä¸­å¿ƒæ”¶é›†çš„ä¸“æœ‰æ•°æ®é›†ã€‚
ä¸¤è€…éƒ½å–å¾—äº†æ›´å¥½çš„BLEUå¾—åˆ†è¶…è¿‡å¼ºå¤§çš„åŸºçº¿ã€‚

2021-05-26

[ğŸŒŸğŸŒŸ]Multi-Task Learning of Generation and Classification for Emotion-Aware  Dialogue Response Generation
æ ‡é¢˜ï¼šæƒ…æ„Ÿæ„ŸçŸ¥å¯¹è¯å“åº”ç”Ÿæˆå’Œåˆ†ç±»çš„å¤šä»»åŠ¡å­¦ä¹ 
ä½œè€…ï¼šTatsuya Ide,Daisuke Kawahara
æœºæ„ï¼šWaseda University
å¤‡æ³¨ï¼šNAACL Student Research Workshop (SRW) 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.11696
æ‘˜è¦ï¼šè®¡ç®—æœºè¦è‡ªç„¶åœ°ä¸äººäº’åŠ¨ï¼Œå°±å¿…é¡»åƒäººä¸€æ ·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæƒ…æ„Ÿçš„å¤šä»»åŠ¡ç”Ÿæˆåˆ†ç±»å­¦ä¹ ç¥ç»ååº”ç”Ÿæˆæ¨¡å‹ã€‚
æˆ‘ä»¬çš„æ¨¡å‹åŸºäºBARTï¼ˆLewisç­‰äººï¼Œ2020ï¼‰ï¼Œä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„Transformer-ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œè¢«è®­ç»ƒæ¥åŒæ—¶äº§ç”Ÿååº”å’Œè¯†åˆ«æƒ…ç»ªã€‚
æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä»»åŠ¡çš„æŸå¤±è¿›è¡ŒåŠ æƒï¼Œä»¥æ§åˆ¶å‚æ•°çš„æ›´æ–°ã€‚è‡ªåŠ¨è¯„ä¼°å’Œä¼—åŒ…æ‰‹åŠ¨è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä½¿ç”Ÿæˆçš„ååº”æ›´å…·æƒ…æ„Ÿæ„è¯†ã€‚

[ğŸŒŸğŸŒŸ]Language Model as an Annotator: Exploring DialoGPT for Dialogue  Summarization
æ ‡é¢˜ï¼šä½œä¸ºæ³¨é‡Šå™¨çš„è¯­è¨€æ¨¡å‹ï¼šå¯¹è¯æ‘˜è¦çš„DialoGPTæ¢ç´¢
ä½œè€…ï¼šXiachong Feng,Xiaocheng Feng,Libo Qin,Bing Qin,Ting Liu
æœºæ„ï¼šResearch Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China
å¤‡æ³¨ï¼šACL 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.12544
æ‘˜è¦ï¼šå½“å‰çš„å¯¹è¯æ‘˜è¦ç³»ç»Ÿé€šå¸¸ä½¿ç”¨ä¸€äº›é€šç”¨çš„è¯­ä¹‰ç‰¹å¾ï¼ˆå¦‚å…³é”®å­—å’Œä¸»é¢˜ï¼‰å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œä»¥è·å¾—æ›´å¼ºå¤§çš„å¯¹è¯å»ºæ¨¡èƒ½åŠ›ã€‚
ç„¶è€Œï¼Œè¿™äº›ç‰¹æ€§æ˜¯é€šè¿‡å¼€æ”¾åŸŸå·¥å…·ç®±è·å¾—çš„ï¼Œè¿™äº›å·¥å…·ç®±ä¸å¯¹è¯æ¡†æ— å…³æˆ–ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šã€‚
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†DialoGPTï¼Œä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„ä¼šè¯ååº”ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚ä½•åˆ©ç”¨DialoGPTä¸­ç¼–ç çš„å¯¹è¯èƒŒæ™¯çŸ¥è¯†ï¼Œå‘å±•æˆä¸€ä¸ªæ— ç›‘ç£çš„å¯¹è¯æ³¨é‡Šå™¨ã€‚
æˆ‘ä»¬ä½¿ç”¨DialoGPTåœ¨SAMSumå’ŒAMIä¸¤ä¸ªå¯¹è¯æ‘˜è¦æ•°æ®é›†ä¸Šæ ‡è®°ä¸‰ç§ç±»å‹çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒå’Œéé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæˆ‘ä»¬çš„æ€»ç»“ã€‚
å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ç§æ•°æ®é›†ä¸Šéƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨SAMSumæ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æ€§èƒ½ã€‚
