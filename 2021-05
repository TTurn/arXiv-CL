2021-05-10

[ğŸŒŸ]Are Pre-trained Convolutions Better than Pre-trained Transformers?
æ ‡é¢˜ï¼šé¢„å…ˆè®­ç»ƒçš„å·ç§¯æ¯”é¢„å…ˆè®­ç»ƒçš„Transformeræ›´å¥½å—ï¼Ÿ
ä½œè€…ï¼šYi Tay,Mostafa Dehghani,Jai Gupta,Dara Bahri,Vamsi Aribandi,Zhen Qin,Donald Metzler
æœºæ„ï¼šGoogle Research, Brain Team, Amsterdam, Netherlands, Mountain View, California Mountain View, California Mountain View, California, Google research, Mountain view, California
å¤‡æ³¨ï¼šAccepted to ACL 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.03322
åŸºäºå·ç§¯çš„seq2seqèŒƒå¼çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ•ˆæœä¸è¾“tansformerï¼Œé€Ÿåº¦æ›´ä¼˜ã€‚

[ğŸŒŸğŸŒŸ]A Survey of Data Augmentation Approaches for NLP
æ ‡é¢˜ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ•°æ®å¢å¼ºæ–¹æ³•ç»¼è¿°
ä½œè€…ï¼šSteven Y. Feng,Varun Gangal,Jason Wei,Sarath Chandar,Soroush Vosoughi,Teruko Mitamura,Eduard Hovy
æœºæ„ï¼šCarnegie Mellon University,Google Research, Mila-Quebec AI Institute,Dartmouth College, soroushadartmouth. edu
å¤‡æ³¨ï¼šAccepted to ACL 2021 Findings
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.03075

2021-05-11

[ğŸŒŸğŸŒŸğŸŒŸ]ReadTwice: Reading Very Large Documents with Memories
æ ‡é¢˜ï¼šReadTwiceï¼šç”¨è®°å¿†é˜…è¯»éå¸¸å¤§çš„æ–‡æ¡£
ä½œè€…ï¼šYury Zemlyanskiy,Joshua Ainslie,Michiel de Jong,Philip Pham,Ilya Eckstein,Fei Sha
æœºæ„ï¼šUniversity of Southern California, Google Research
å¤‡æ³¨ï¼šTo appear in the proceedings of NAACL 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.04241
ä½¿ç”¨ä¸¤æ­¥èµ°çš„æ–¹æ¡ˆè¿›è¡Œé˜…è¯»ç†è§£æˆ–è€…æ˜¯QAï¼Œç¬¬ä¸€æ­¥ä½¿ç”¨è®°å¿†æ¨¡å—è®°å½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¬¬äºŒæ­¥åœ¨å…¶ä»–æ®µè½çš„æ—¶å€™ä½¿ç”¨ã€‚

[ğŸŒŸğŸŒŸ]Poolingformer: Long Document Modeling with Pooling Attention
æ ‡é¢˜ï¼šPoolingFormï¼šå…·æœ‰é›†ä¸­æ³¨æ„åŠ›çš„é•¿æ–‡æ¡£å»ºæ¨¡
ä½œè€…ï¼šHang Zhang,Yeyun Gong,Yelong Shen,Weisheng Li,Jiancheng Lv,Nan Duan,Weizhu Chen
å¤‡æ³¨ï¼šAccepted by ICML 2021
é“¾æ¥ï¼šhttps://arxiv.org/abs/2105.04371
å¸¦æœ‰æ³¨æ„åŠ›poolingçš„é•¿æ–‡æ¡£å»ºæ¨¡é—®ç­”æ¨¡å‹ã€‚
