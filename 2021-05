2021-05-10

[🌟]Are Pre-trained Convolutions Better than Pre-trained Transformers?
标题：预先训练的卷积比预先训练的Transformer更好吗？
作者：Yi Tay,Mostafa Dehghani,Jai Gupta,Dara Bahri,Vamsi Aribandi,Zhen Qin,Donald Metzler
机构：Google Research, Brain Team, Amsterdam, Netherlands, Mountain View, California Mountain View, California Mountain View, California, Google research, Mountain view, California
备注：Accepted to ACL 2021
链接：https://arxiv.org/abs/2105.03322
基于卷积的seq2seq范式的预训练模型，效果不输tansformer，速度更优。

[🌟🌟]A Survey of Data Augmentation Approaches for NLP
标题：自然语言处理中的数据增强方法综述
作者：Steven Y. Feng,Varun Gangal,Jason Wei,Sarath Chandar,Soroush Vosoughi,Teruko Mitamura,Eduard Hovy
机构：Carnegie Mellon University,Google Research, Mila-Quebec AI Institute,Dartmouth College, soroushadartmouth. edu
备注：Accepted to ACL 2021 Findings
链接：https://arxiv.org/abs/2105.03075
https://blog.csdn.net/choose_c/article/details/116769154

2021-05-11

[🌟🌟🌟]ReadTwice: Reading Very Large Documents with Memories
标题：ReadTwice：用记忆阅读非常大的文档
作者：Yury Zemlyanskiy,Joshua Ainslie,Michiel de Jong,Philip Pham,Ilya Eckstein,Fei Sha
机构：University of Southern California, Google Research
备注：To appear in the proceedings of NAACL 2021
链接：https://arxiv.org/abs/2105.04241
使用两步走的方案进行阅读理解或者是QA，第一步使用记忆模块记录上下文信息，第二步在其他段落的时候使用。

[🌟🌟]Poolingformer: Long Document Modeling with Pooling Attention
标题：PoolingForm：具有集中注意力的长文档建模
作者：Hang Zhang,Yeyun Gong,Yelong Shen,Weisheng Li,Jiancheng Lv,Nan Duan,Weizhu Chen
备注：Accepted by ICML 2021
链接：https://arxiv.org/abs/2105.04371
带有注意力pooling的长文档建模问答模型。

2021-05-17

[🌟]Locate and Label: A Two-stage Identifier for Nested Named Entity  Recognition
标题：定位和标注：嵌套命名实体识别的两阶段标识符
作者：Yongliang Shen,Xinyin Ma,Zeqi Tan,Shuai Zhang,Wen Wang,Weiming Lu
机构：College of Computer Science and Technology, Zhejiang University, University of Science and Technology of China
备注：Accepted to ACL 2021, submission version
链接：https://arxiv.org/abs/2105.06804
摘要：命名实体识别（Named entity recognition，NER）是自然语言处理中的一个研究热点。传统的NER研究只涉及平面实体，忽略了嵌套实体。
基于广域的方法将实体识别视为广域分类任务。这些方法虽然具有处理嵌套NER的能力，但计算量大，对边界信息的忽略，对部分匹配实体的跨度利用不足，长实体识别困难。
为了解决这些问题，我们提出了一种两阶段实体标识符。首先通过对种子跨度进行过滤和边界回归来生成跨度建议以定位实体，然后用相应的类别标记边界调整后的跨度建议。
该方法有效地利用了训练过程中实体和部分匹配跨度的边界信息。通过边界回归，理论上可以覆盖任意长度的实体，提高了对长实体的识别能力。
此外，在第一阶段中过滤掉许多低质量的种子跨度，降低了推理的时间复杂度。在嵌套的NER数据集上的实验表明，本文提出的方法优于现有的模型。

2021-05-18

[🌟]Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter
标题：基于BERT适配器的词典增强型中文序列标注
作者：Wei Liu,Xiyan Fu,Yue Zhang,Wenming Xiao
机构：DAMO Academy, Alibaba Group, China, College of Computer Science, Nankai University, China, School of Engineering, Westlake University, China, Institute of Advanced Technology, Westlake Institute for Advanced Study
备注：accepted by ACL2021
链接：https://arxiv.org/abs/2105.07148
摘要：由于词库信息和预训练模型（如BERT）各自的优势，它们被用来探索汉语序列标注任务。然而，现有的方法仅仅通过一个浅层随机初始化的序列层来融合词汇特征，而没有将它们集成到BERT的底层。
本文提出了一种用于中文序列标注的词库增强BERT（LEBERT），它通过词库适配器层将外部词库知识直接集成到BERT层。与现有方法相比，该模型有利于在BERT的底层进行深层词汇知识融合。
在十个中文数据集上进行了命名实体识别、分词和词性标注三个任务的实验，实验结果表明LEBERT取得了最新的结果。

2021-05-20

[🌟]Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation
标题：用于闭域对话生成的检索-增强型Transformer-XL
作者：Giovanni Bonetta,Rossella Cancelliere,Ding Liu,Paul Vozila
机构：Department of Computer Science, University of Turin, Torino, Italy, Nuance Communications Inc., Burlington, MA, USA
备注：The International FLAIRS Conference Proceedings volume 34 issue 1
链接：https://arxiv.org/abs/2105.09235
摘要：基于Transformer的模型在自然语言生成中表现出很好的模式和结构捕获能力，并在许多任务中取得了最新的成果。本文提出了一种基于Transformer的多匝对话响应生成模型。
我们的解决方案基于一种混合方法，该方法通过一种新的检索机制来扩充基于Transformer的生成模型，该机制通过k近邻搜索来利用训练数据中的记忆信息。
我们的系统是在两个由客户/助理对话框制作的数据集上进行评估的：Taskmaster-1，由Google发布，拥有高质量、面向目标的会话数据和从真实的客户服务呼叫中心收集的专有数据集。
两者都取得了更好的BLEU得分超过强大的基线。

2021-05-26

[🌟🌟]Multi-Task Learning of Generation and Classification for Emotion-Aware  Dialogue Response Generation
标题：情感感知对话响应生成和分类的多任务学习
作者：Tatsuya Ide,Daisuke Kawahara
机构：Waseda University
备注：NAACL Student Research Workshop (SRW) 2021
链接：https://arxiv.org/abs/2105.11696
摘要：计算机要自然地与人互动，就必须像人一样。本文提出了一种基于情感的多任务生成分类学习神经反应生成模型。
我们的模型基于BART（Lewis等人，2020），一个预先训练的Transformer-编码器-解码器模型，被训练来同时产生反应和识别情绪。
此外，我们对任务的损失进行加权，以控制参数的更新。自动评估和众包手动评估表明，该模型使生成的反应更具情感意识。

[🌟🌟]Language Model as an Annotator: Exploring DialoGPT for Dialogue  Summarization
标题：作为注释器的语言模型：对话摘要的DialoGPT探索
作者：Xiachong Feng,Xiaocheng Feng,Libo Qin,Bing Qin,Ting Liu
机构：Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China
备注：ACL 2021
链接：https://arxiv.org/abs/2105.12544
摘要：当前的对话摘要系统通常使用一些通用的语义特征（如关键字和主题）对文本进行编码，以获得更强大的对话建模能力。
然而，这些特性是通过开放域工具箱获得的，这些工具箱与对话框无关或严重依赖于人工注释。
在本文中，我们展示了DialoGPT，一个预先训练好的会话反应生成模型，如何利用DialoGPT中编码的对话背景知识，发展成一个无监督的对话注释器。
我们使用DialoGPT在SAMSum和AMI两个对话摘要数据集上标记三种类型的特征，并使用预训练和非预训练模型作为我们的总结。
实验结果表明，该方法在两种数据集上都有显著的改进，在SAMSum数据集上取得了新的性能。

2021-05-31

[🌟🌟]Data Augmentation for Text Generation Without Any Augmented Data
标题：用于无任何扩充数据的文本生成的数据扩充
作者：Wei Bi,Huayang Li,Jiacheng Huang
机构：Tencent AI Lab, Shenzhen, China
备注：Accepted into the main conference of ACL 2021
链接：https://arxiv.org/abs/2105.13650
摘要：数据扩充是提高许多神经文本生成模型性能的有效途径。然而，现有的数据增广方法需要定义或选择合适的数据映射函数，将原始样本映射到增广样本中。
在这项工作中，我们提出了一个目标来描述文本生成任务中的数据扩充问题，而不需要使用由特定映射函数构造的扩充数据。
在保证收敛速度的前提下，我们提出的目标可以有效地优化并应用于文本生成任务中常见的损失函数。
在两个文本生成任务的五个数据集上的实验表明，该方法可以接近甚至超过流行的数据扩充方法。
