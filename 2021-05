2021-05-10

[]Are Pre-trained Convolutions Better than Pre-trained Transformers?
标题：预先训练的卷积比预先训练的Transformer更好吗？
作者：Yi Tay,Mostafa Dehghani,Jai Gupta,Dara Bahri,Vamsi Aribandi,Zhen Qin,Donald Metzler
机构：Google Research, Brain Team, Amsterdam, Netherlands, Mountain View, California Mountain View, California Mountain View, California, Google research, Mountain view, California
备注：Accepted to ACL 2021
链接：https://arxiv.org/abs/2105.03322

[]A Survey of Data Augmentation Approaches for NLP
标题：自然语言处理中的数据增强方法综述
作者：Steven Y. Feng,Varun Gangal,Jason Wei,Sarath Chandar,Soroush Vosoughi,Teruko Mitamura,Eduard Hovy
机构：Carnegie Mellon University,Google Research, Mila-Quebec AI Institute,Dartmouth College, soroushadartmouth. edu
备注：Accepted to ACL 2021 Findings
链接：https://arxiv.org/abs/2105.03075
