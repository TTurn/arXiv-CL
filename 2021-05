2021-05-10

[🌟]Are Pre-trained Convolutions Better than Pre-trained Transformers?
标题：预先训练的卷积比预先训练的Transformer更好吗？
作者：Yi Tay,Mostafa Dehghani,Jai Gupta,Dara Bahri,Vamsi Aribandi,Zhen Qin,Donald Metzler
机构：Google Research, Brain Team, Amsterdam, Netherlands, Mountain View, California Mountain View, California Mountain View, California, Google research, Mountain view, California
备注：Accepted to ACL 2021
链接：https://arxiv.org/abs/2105.03322
基于卷积的seq2seq范式的预训练模型，效果不输tansformer，速度更优。

[🌟🌟]A Survey of Data Augmentation Approaches for NLP
标题：自然语言处理中的数据增强方法综述
作者：Steven Y. Feng,Varun Gangal,Jason Wei,Sarath Chandar,Soroush Vosoughi,Teruko Mitamura,Eduard Hovy
机构：Carnegie Mellon University,Google Research, Mila-Quebec AI Institute,Dartmouth College, soroushadartmouth. edu
备注：Accepted to ACL 2021 Findings
链接：https://arxiv.org/abs/2105.03075

2021-05-11

[🌟🌟🌟]ReadTwice: Reading Very Large Documents with Memories
标题：ReadTwice：用记忆阅读非常大的文档
作者：Yury Zemlyanskiy,Joshua Ainslie,Michiel de Jong,Philip Pham,Ilya Eckstein,Fei Sha
机构：University of Southern California, Google Research
备注：To appear in the proceedings of NAACL 2021
链接：https://arxiv.org/abs/2105.04241
使用两步走的方案进行阅读理解或者是QA，第一步使用记忆模块记录上下文信息，第二步在其他段落的时候使用。

[🌟🌟]Poolingformer: Long Document Modeling with Pooling Attention
标题：PoolingForm：具有集中注意力的长文档建模
作者：Hang Zhang,Yeyun Gong,Yelong Shen,Weisheng Li,Jiancheng Lv,Nan Duan,Weizhu Chen
备注：Accepted by ICML 2021
链接：https://arxiv.org/abs/2105.04371
带有注意力pooling的长文档建模问答模型。

2021-05-17

[🌟]Locate and Label: A Two-stage Identifier for Nested Named Entity  Recognition
标题：定位和标注：嵌套命名实体识别的两阶段标识符
作者：Yongliang Shen,Xinyin Ma,Zeqi Tan,Shuai Zhang,Wen Wang,Weiming Lu
机构：College of Computer Science and Technology, Zhejiang University, University of Science and Technology of China
备注：Accepted to ACL 2021, submission version
链接：https://arxiv.org/abs/2105.06804
摘要：命名实体识别（Named entity recognition，NER）是自然语言处理中的一个研究热点。传统的NER研究只涉及平面实体，忽略了嵌套实体。
基于广域的方法将实体识别视为广域分类任务。这些方法虽然具有处理嵌套NER的能力，但计算量大，对边界信息的忽略，对部分匹配实体的跨度利用不足，长实体识别困难。
为了解决这些问题，我们提出了一种两阶段实体标识符。首先通过对种子跨度进行过滤和边界回归来生成跨度建议以定位实体，然后用相应的类别标记边界调整后的跨度建议。
该方法有效地利用了训练过程中实体和部分匹配跨度的边界信息。通过边界回归，理论上可以覆盖任意长度的实体，提高了对长实体的识别能力。
此外，在第一阶段中过滤掉许多低质量的种子跨度，降低了推理的时间复杂度。在嵌套的NER数据集上的实验表明，本文提出的方法优于现有的模型。

2021-05-18

[🌟]Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter
标题：基于BERT适配器的词典增强型中文序列标注
作者：Wei Liu,Xiyan Fu,Yue Zhang,Wenming Xiao
机构：DAMO Academy, Alibaba Group, China, College of Computer Science, Nankai University, China, School of Engineering, Westlake University, China, Institute of Advanced Technology, Westlake Institute for Advanced Study
备注：accepted by ACL2021
链接：https://arxiv.org/abs/2105.07148
摘要：由于词库信息和预训练模型（如BERT）各自的优势，它们被用来探索汉语序列标注任务。然而，现有的方法仅仅通过一个浅层随机初始化的序列层来融合词汇特征，而没有将它们集成到BERT的底层。
本文提出了一种用于中文序列标注的词库增强BERT（LEBERT），它通过词库适配器层将外部词库知识直接集成到BERT层。与现有方法相比，该模型有利于在BERT的底层进行深层词汇知识融合。
在十个中文数据集上进行了命名实体识别、分词和词性标注三个任务的实验，实验结果表明LEBERT取得了最新的结果。

2021-05-20

[🌟]Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation
标题：用于闭域对话生成的检索-增强型Transformer-XL
作者：Giovanni Bonetta,Rossella Cancelliere,Ding Liu,Paul Vozila
机构：Department of Computer Science, University of Turin, Torino, Italy, Nuance Communications Inc., Burlington, MA, USA
备注：The International FLAIRS Conference Proceedings volume 34 issue 1
链接：https://arxiv.org/abs/2105.09235
摘要：基于Transformer的模型在自然语言生成中表现出很好的模式和结构捕获能力，并在许多任务中取得了最新的成果。本文提出了一种基于Transformer的多匝对话响应生成模型。
我们的解决方案基于一种混合方法，该方法通过一种新的检索机制来扩充基于Transformer的生成模型，该机制通过k近邻搜索来利用训练数据中的记忆信息。
我们的系统是在两个由客户/助理对话框制作的数据集上进行评估的：Taskmaster-1，由Google发布，拥有高质量、面向目标的会话数据和从真实的客户服务呼叫中心收集的专有数据集。
两者都取得了更好的BLEU得分超过强大的基线。
