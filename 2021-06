2021-06-02

[🌟]Training ELECTRA Augmented with Multi-word Selection
标题：使用多词选择增强的训练Electra
作者：Jiaming Shen,Jialu Liu,Tianqi Liu,Cong Yu,Jiawei Han
机构：∇University of Illinois Urbana-Champaign, IL, USA,Google Research, NY, USA
备注：Accepted in Findings of ACL 2021
链接：https://arxiv.org/abs/2106.00139
摘要：预先训练的文本编码器，如BERT及其变体，最近在许多NLP任务上取得了最先进的性能。
这些预训练方法虽然有效，但通常需要大量的计算资源。为了加速预训练，ELECTRA训练了一个鉴别器，该鉴别器预测每个输入令牌是否被生成器替换。
然而，这个新的任务，作为一个二进制分类，语义信息较少。本文提出了一种基于多任务学习的文本编码器预训练方法，对ELECTRA算法进行了改进。
具体地说，我们训练鉴别器同时检测替换的令牌并从候选集合中选择原始令牌。我们进一步开发了两种技术来有效地结合所有预训练任务：
（1）使用基于注意的网络来处理特定任务的头部；（2）共享生成器和鉴别器的底层。在GLUE和SQuAD数据集上的大量实验证明了该方法的有效性和有效性。
